{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and Classifying Livestock Farming Profiles to Identify Agricultural Vulnerability in Nepal\n",
    "\n",
    "## A Machine Learning Approach\n",
    "\n",
    "**Author:** ML Assignment Project  \n",
    "**Date:** December 2025\n",
    "\n",
    "---\n",
    "\n",
    "### Background\n",
    "\n",
    "Livestock farming is an essential part of Nepal's rural economy, providing income, nutrition, and a critical social safety net for millions of households. However, the agricultural landscape is not uniform; farming systems vary dramatically across the diverse ecological zones of the country (Terai, Hills, and Mountains).\n",
    "\n",
    "This study applies machine learning techniques to a nationally representative dataset to:\n",
    "1. **Identify and characterize** distinct livestock farming profiles using K-Means clustering\n",
    "2. **Develop an interpretable classification model** using Decision Trees to understand vulnerability drivers\n",
    "\n",
    "### Data Source\n",
    "National Sample Census of Agriculture 2021-22 (NSCA 2078), National Statistics Office (NSO), Government of Nepal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Data Loading\n",
    "\n",
    "### 1.1 Install Required Libraries (for Google Colab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if running in Google Colab)\n",
    "# !pip install geopandas folium mapclassify -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Core libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    silhouette_score\n",
    ")\n",
    "\n",
    "# Geographic visualization (optional - will handle gracefully if not available)\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    GEOPANDAS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GEOPANDAS_AVAILABLE = False\n",
    "    print(\"Note: geopandas not available. Map visualizations will be skipped.\")\n",
    "    print(\"Install with: pip install geopandas\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"\u2713 All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load Data (Google Colab Compatible)\n",
    "\n",
    "**Option A:** Upload files directly (recommended for Colab)  \n",
    "**Option B:** Mount Google Drive and load from there  \n",
    "**Option C:** Load from local path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment and set up data loading\n",
    "import os\n",
    "\n",
    "# Check if running in Google Colab\n",
    "IN_COLAB = 'google.colab' in str(get_ipython()) if 'get_ipython' in dir() else False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab environment\")\n",
    "else:\n",
    "    print(\"Running in local environment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "# Update these paths according to your setup\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Select/upload CSV files in Colab (file picker)\n",
    "    from google.colab import files\n",
    "    print(\"Please select the three CSV files (you can multi-select):\")\n",
    "    print(\"  - table-1.-number-of-holdings-and-area-by-district-.csv\")\n",
    "    print(\"  - table-4.1.-number-area-number-of-holdings-reporting-and-area-irrigated-by-source-of-irrigation-a.csv\")\n",
    "    print(\"  - table-17.-number-of-holdings-livestocks-and-poultry-by-districts.csv\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    def pick_file(keyword, fallback=None):\n",
    "        for fname in uploaded.keys():\n",
    "            if keyword.lower() in fname.lower():\n",
    "                return fname\n",
    "        if fallback and fallback in uploaded:\n",
    "            return fallback\n",
    "        # default to first uploaded\n",
    "        return list(uploaded.keys())[0]\n",
    "\n",
    "    holdings_area_path = pick_file('holdings-and-area', 'table-1.-number-of-holdings-and-area-by-district-.csv')\n",
    "    irrigation_path = pick_file('irrigation', 'table-4.1.-number-area-number-of-holdings-reporting-and-area-irrigated-by-source-of-irrigation-a.csv')\n",
    "    livestock_path = pick_file('livestocks-and-poultry', 'table-17.-number-of-holdings-livestocks-and-poultry-by-districts.csv')\n",
    "\n",
    "    print(f\"Using files -> holdings: {holdings_area_path}, irrigation: {irrigation_path}, livestock: {livestock_path}\")\n",
    "else:\n",
    "    # Local paths - update as needed\n",
    "    base_path = '/Users/eklakdangaura/College/ML/Assignment/Datasets/'\n",
    "    holdings_area_path = base_path + 'table-1.-number-of-holdings-and-area-by-district-.csv'\n",
    "    irrigation_path = base_path + 'table-4.1.-number-area-number-of-holdings-reporting-and-area-irrigated-by-source-of-irrigation-a.csv'\n",
    "    livestock_path = base_path + 'table-17.-number-of-holdings-livestocks-and-poultry-by-districts.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three datasets\n",
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "# Dataset 1: Holdings and Area by District\n",
    "df_holdings = pd.read_csv(holdings_area_path, sep='\\t')\n",
    "print(f\"1. Holdings & Area Dataset: {df_holdings.shape[0]} rows, {df_holdings.shape[1]} columns\")\n",
    "\n",
    "# Dataset 2: Irrigation Data\n",
    "df_irrigation = pd.read_csv(irrigation_path, sep='\\t')\n",
    "print(f\"2. Irrigation Dataset: {df_irrigation.shape[0]} rows, {df_irrigation.shape[1]} columns\")\n",
    "\n",
    "# Dataset 3: Livestock Data\n",
    "df_livestock = pd.read_csv(livestock_path)\n",
    "print(f\"3. Livestock Dataset: {df_livestock.shape[0]} rows, {df_livestock.shape[1]} columns\")\n",
    "\n",
    "print(\"\\n\u2713 All datasets loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Explore Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Holdings & Area Dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET 1: Holdings and Area by District\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nColumns:\", df_holdings.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df_holdings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Irrigation Dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET 2: Irrigation Data\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nColumns:\", df_irrigation.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df_irrigation.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Livestock Dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET 3: Livestock Data\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nColumns:\", df_livestock.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df_livestock.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Preprocessing and Feature Engineering\n",
    "\n",
    "### 2.1 Clean and Standardize District Names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean district names for consistent merging\n",
    "def clean_district_name(name):\n",
    "    \"\"\"Standardize district names by removing whitespace and converting to lowercase.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return str(name).strip().lower()\n",
    "\n",
    "# Our dataset uses these district names - we keep them as-is\n",
    "# The mapping to GADM names happens later in the thematic map section\n",
    "# This ensures our data processing is independent of the shapefile naming\n",
    "\n",
    "def standardize_district_name(name):\n",
    "    \"\"\"Clean district names (lowercase, stripped).\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return str(name).strip().lower()\n",
    "\n",
    "# Clean district names in all datasets\n",
    "# Dataset 1: Holdings\n",
    "df_holdings['district_clean'] = df_holdings['Districts'].apply(standardize_district_name)\n",
    "\n",
    "# Dataset 2: Irrigation\n",
    "df_irrigation['district_clean'] = df_irrigation['Districts'].apply(standardize_district_name)\n",
    "\n",
    "# Dataset 3: Livestock\n",
    "df_livestock['district_clean'] = df_livestock['District'].apply(standardize_district_name)\n",
    "\n",
    "# Display unique district counts\n",
    "print(f\"Holdings dataset: {df_holdings['district_clean'].nunique()} districts\")\n",
    "print(f\"Irrigation dataset: {df_irrigation['district_clean'].nunique()} districts\")\n",
    "print(f\"Livestock dataset: {df_livestock['district_clean'].nunique()} districts\")\n",
    "\n",
    "# Show sample district names\n",
    "print(\"\\nSample district names from our dataset:\")\n",
    "sample_districts = sorted(df_holdings['district_clean'].dropna().unique())[:10]\n",
    "for d in sample_districts:\n",
    "    print(f\"  - {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any district name mismatches\n",
    "holdings_districts = set(df_holdings['district_clean'].dropna())\n",
    "irrigation_districts = set(df_irrigation['district_clean'].dropna())\n",
    "livestock_districts = set(df_livestock['district_clean'].dropna())\n",
    "\n",
    "# Find common districts\n",
    "common_districts = holdings_districts & irrigation_districts & livestock_districts\n",
    "print(f\"Common districts across all datasets: {len(common_districts)}\")\n",
    "\n",
    "# Check for mismatches\n",
    "all_districts = holdings_districts | irrigation_districts | livestock_districts\n",
    "if len(all_districts) != len(common_districts):\n",
    "    print(\"\\nDistricts only in specific datasets (potential mismatches):\")\n",
    "    only_holdings = holdings_districts - common_districts\n",
    "    only_irrigation = irrigation_districts - common_districts\n",
    "    only_livestock = livestock_districts - common_districts\n",
    "    \n",
    "    if only_holdings:\n",
    "        print(f\"  Only in Holdings: {only_holdings}\")\n",
    "    if only_irrigation:\n",
    "        print(f\"  Only in Irrigation: {only_irrigation}\")\n",
    "    if only_livestock:\n",
    "        print(f\"  Only in Livestock: {only_livestock}\")\n",
    "    \n",
    "    # Try to identify potential matches\n",
    "    print(\"\\n\u26a0\ufe0f Attempting to fix district name mismatches...\")\n",
    "    print(\"If mismatches persist, update the 'district_name_fixes' dictionary above.\")\n",
    "else:\n",
    "    print(\"\\n\u2713 All districts match across datasets!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Merge Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns from each dataset before merging\n",
    "\n",
    "# From Holdings dataset\n",
    "df_holdings_select = df_holdings[[\n",
    "    'district_clean', 'Districts', 'Number of holdings', \n",
    "    'Total wet  area (ha)', 'Total dry  area (ha)', 'Total  area (ha)'\n",
    "]].copy()\n",
    "df_holdings_select.columns = [\n",
    "    'district_clean', 'district_name', 'num_holdings',\n",
    "    'wet_area_ha', 'dry_area_ha', 'total_area_ha'\n",
    "]\n",
    "\n",
    "# From Irrigation dataset\n",
    "df_irrigation_select = df_irrigation[[\n",
    "    'district_clean', 'No. of holdings reporting irrigation', \n",
    "    'Total area (ha) of irrigation'\n",
    "]].copy()\n",
    "df_irrigation_select.columns = [\n",
    "    'district_clean', 'holdings_with_irrigation', 'irrigated_area_ha'\n",
    "]\n",
    "\n",
    "# From Livestock dataset - Check column names first\n",
    "print(\"Livestock dataset columns:\")\n",
    "print(df_livestock.columns.tolist())\n",
    "\n",
    "# Select livestock columns (handling potential column name variations)\n",
    "livestock_cols = ['district_clean', 'Total number of holdings', 'Number of holdings reporting livestock',\n",
    "                  'No. of  cattles', 'No. of buffalo', 'No. of goat/chyangra', \n",
    "                  'No. of pigs/boar', 'No. of poultry(chicken)', 'No. of sheep']\n",
    "\n",
    "df_livestock_select = df_livestock[livestock_cols].copy()\n",
    "df_livestock_select.columns = [\n",
    "    'district_clean', 'total_holdings_livestock', 'holdings_with_livestock',\n",
    "    'num_cattle', 'num_buffalo', 'num_goats', 'num_pigs', 'num_poultry', 'num_sheep'\n",
    "]\n",
    "\n",
    "# Handle missing values in sheep column (some districts may not report sheep)\n",
    "print(f\"\\nMissing values in num_sheep before fix: {df_livestock_select['num_sheep'].isna().sum()}\")\n",
    "df_livestock_select['num_sheep'] = pd.to_numeric(df_livestock_select['num_sheep'], errors='coerce').fillna(0)\n",
    "print(f\"Missing values in num_sheep after fix: {df_livestock_select['num_sheep'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nSelected columns from each dataset:\")\n",
    "print(f\"  Holdings: {df_holdings_select.shape}\")\n",
    "print(f\"  Irrigation: {df_irrigation_select.shape}\")\n",
    "print(f\"  Livestock: {df_livestock_select.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all datasets on district_clean\n",
    "# Using outer join first to identify any missing districts, then filtering\n",
    "df_merged_outer = df_holdings_select.merge(\n",
    "    df_irrigation_select, on='district_clean', how='outer'\n",
    ").merge(\n",
    "    df_livestock_select, on='district_clean', how='outer'\n",
    ")\n",
    "\n",
    "# Check for districts with missing data after merge\n",
    "print(\"Districts with missing data after merge:\")\n",
    "missing_data = df_merged_outer[df_merged_outer.isnull().any(axis=1)]\n",
    "if len(missing_data) > 0:\n",
    "    print(f\"  Found {len(missing_data)} districts with incomplete data:\")\n",
    "    for _, row in missing_data.iterrows():\n",
    "        missing_cols = row[row.isnull()].index.tolist()\n",
    "        print(f\"    - {row['district_clean']}: missing {missing_cols}\")\n",
    "    \n",
    "    # Fill missing numeric values with 0 (for districts like Pyuthan with missing livestock data)\n",
    "    print(\"\\n  Filling missing values with 0 for numeric columns...\")\n",
    "    numeric_cols = ['num_holdings', 'wet_area_ha', 'dry_area_ha', 'total_area_ha',\n",
    "                    'holdings_with_irrigation', 'irrigated_area_ha',\n",
    "                    'total_holdings_livestock', 'holdings_with_livestock',\n",
    "                    'num_cattle', 'num_buffalo', 'num_goats', 'num_pigs', 'num_poultry', 'num_sheep']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_merged_outer.columns:\n",
    "            df_merged_outer[col] = pd.to_numeric(df_merged_outer[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Fill missing district_name from other sources if available\n",
    "    if 'district_name' in df_merged_outer.columns:\n",
    "        df_merged_outer['district_name'] = df_merged_outer['district_name'].fillna(df_merged_outer['district_clean'])\n",
    "else:\n",
    "    print(\"  \u2713 No missing data found!\")\n",
    "\n",
    "# Use the filled outer join result\n",
    "df_merged = df_merged_outer.dropna(subset=['district_clean'])\n",
    "\n",
    "print(f\"\\nMerged dataset shape: {df_merged.shape}\")\n",
    "print(f\"Number of districts: {df_merged['district_clean'].nunique()}\")\n",
    "print(\"\\nMerged dataset preview:\")\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in merged dataset:\")\n",
    "missing = df_merged.isnull().sum()\n",
    "if missing.any():\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"\u2713 No missing values!\")\n",
    "\n",
    "# Verify num_sheep column is present\n",
    "print(f\"\\n\u2713 Columns in merged dataset: {df_merged.columns.tolist()}\")\n",
    "print(f\"\\n\u2713 num_sheep column present: {'num_sheep' in df_merged.columns}\")\n",
    "if 'num_sheep' in df_merged.columns:\n",
    "    print(f\"  num_sheep stats: min={df_merged['num_sheep'].min()}, max={df_merged['num_sheep'].max()}, mean={df_merged['num_sheep'].mean():.2f}\")\n",
    "\n",
    "# Display data types\n",
    "print(\"\\nData types:\")\n",
    "print(df_merged.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Engineering\n",
    "\n",
    "Create derived features that capture agricultural and livestock characteristics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df = df_merged.copy()\n",
    "\n",
    "# Handle any missing or invalid values\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert numeric columns that might be strings\n",
    "numeric_cols = ['num_holdings', 'wet_area_ha', 'dry_area_ha', 'total_area_ha',\n",
    "                'holdings_with_irrigation', 'irrigated_area_ha',\n",
    "                'num_cattle', 'num_buffalo', 'num_goats', 'num_pigs', 'num_poultry', 'num_sheep']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Drop any rows with NaN after conversion\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Dataset after cleaning: {df.shape[0]} districts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Create derived features\n",
    "print(\"Creating engineered features...\\n\")\n",
    "\n",
    "# 1. Livestock density per holding\n",
    "df['cattle_per_holding'] = df['num_cattle'] / df['num_holdings']\n",
    "df['buffalo_per_holding'] = df['num_buffalo'] / df['num_holdings']\n",
    "df['goat_per_holding'] = df['num_goats'] / df['num_holdings']\n",
    "df['pig_per_holding'] = df['num_pigs'] / df['num_holdings']\n",
    "df['poultry_per_holding'] = df['num_poultry'] / df['num_holdings']\n",
    "df['sheep_per_holding'] = df['num_sheep'] / df['num_holdings']\n",
    "\n",
    "# 2. Total livestock per holding (excluding poultry as it has different scale)\n",
    "df['total_large_livestock'] = df['num_cattle'] + df['num_buffalo'] + df['num_goats'] + df['num_pigs'] + df['num_sheep']\n",
    "df['total_livestock_per_holding'] = df['total_large_livestock'] / df['num_holdings']\n",
    "\n",
    "# 3. Land-related features\n",
    "df['avg_land_holding'] = df['total_area_ha'] / df['num_holdings']  # Average land size per holding\n",
    "df['wet_land_ratio'] = df['wet_area_ha'] / df['total_area_ha']  # Proportion of wet (irrigated potential) land\n",
    "df['irrigated_land_pct'] = (df['irrigated_area_ha'] / df['total_area_ha']) * 100  # % of land actually irrigated\n",
    "\n",
    "# 4. Livestock composition ratios\n",
    "df['cattle_buffalo_ratio'] = df['num_cattle'] / (df['num_buffalo'] + 1)  # Cattle vs Buffalo preference\n",
    "df['small_livestock_ratio'] = (df['num_goats'] + df['num_sheep']) / (df['total_large_livestock'] + 1)  # Small vs large livestock\n",
    "\n",
    "# 5. Agricultural intensity\n",
    "df['irrigation_coverage_pct'] = (df['holdings_with_irrigation'] / df['num_holdings']) * 100\n",
    "df['livestock_density_per_ha'] = df['total_large_livestock'] / df['total_area_ha']  # Livestock per hectare\n",
    "\n",
    "print(\"\u2713 Engineered features created:\")\n",
    "new_features = [\n",
    "    'cattle_per_holding', 'buffalo_per_holding', 'goat_per_holding', \n",
    "    'pig_per_holding', 'poultry_per_holding', 'sheep_per_holding',\n",
    "    'total_livestock_per_holding', 'avg_land_holding', 'wet_land_ratio',\n",
    "    'irrigated_land_pct', 'cattle_buffalo_ratio', 'small_livestock_ratio',\n",
    "    'irrigation_coverage_pct', 'livestock_density_per_ha'\n",
    "]\n",
    "for f in new_features:\n",
    "    print(f\"  - {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics of engineered features\n",
    "print(\"Summary Statistics of Engineered Features:\")\n",
    "print(\"=\" * 60)\n",
    "df[new_features].describe().round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle any infinite values that might have been created\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Final dataset: {df.shape[0]} districts with {df.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Select Features for Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE ENGINEERING VISUALIZATION\n",
    "# =============================================================================\n",
    "print(\"Visualizing Engineered Features...\")\n",
    "\n",
    "# 1. Correlation Heatmap of Engineered Features\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = df[new_features].corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, linewidths=0.5, square=True)\n",
    "plt.title('Correlation Matrix of Engineered Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Distribution of Key Features\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "key_features = ['cattle_per_holding', 'buffalo_per_holding', 'goat_per_holding',\n",
    "                'poultry_per_holding', 'avg_land_holding', 'irrigated_land_pct',\n",
    "                'total_livestock_per_holding', 'wet_land_ratio', 'livestock_density_per_ha']\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    sns.histplot(df[feature], kde=True, ax=axes[idx], color='steelblue')\n",
    "    axes[idx].set_title(feature.replace('_', ' ').title(), fontweight='bold')\n",
    "    axes[idx].set_xlabel('')\n",
    "\n",
    "plt.suptitle('Distribution of Engineered Features', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Feature engineering visualizations complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for clustering analysis\n",
    "# These features capture livestock composition, land characteristics, and agricultural intensity\n",
    "\n",
    "clustering_features = [\n",
    "    'cattle_per_holding',      # Cattle density\n",
    "    'buffalo_per_holding',     # Buffalo density\n",
    "    'goat_per_holding',        # Goat density (small ruminants)\n",
    "    'pig_per_holding',         # Pig density\n",
    "    'poultry_per_holding',     # Poultry density\n",
    "    'avg_land_holding',        # Farm size indicator\n",
    "    'irrigated_land_pct',      # Irrigation infrastructure\n",
    "    'wet_land_ratio',          # Land quality indicator\n",
    "    'livestock_density_per_ha' # Overall livestock intensity\n",
    "]\n",
    "\n",
    "# Create feature matrix\n",
    "X = df[clustering_features].copy()\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"\\nFeatures selected for clustering:\")\n",
    "for i, f in enumerate(clustering_features, 1):\n",
    "    print(f\"  {i}. {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE SCALING\n",
    "# =============================================================================\n",
    "# Scale features using StandardScaler\n",
    "# This is crucial for K-Means as it uses distance-based calculations\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=clustering_features, index=X.index)\n",
    "\n",
    "print(\"\u2713 Features scaled using StandardScaler\")\n",
    "print(f\"\\nOriginal feature ranges:\")\n",
    "for col in clustering_features:\n",
    "    print(f\"  {col}: [{X[col].min():.2f}, {X[col].max():.2f}]\")\n",
    "\n",
    "print(f\"\\nScaled feature ranges (should be ~mean=0, std=1):\")\n",
    "print(X_scaled_df.describe().round(2).loc[['mean', 'std']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Part 1: K-Means Clustering for Profile Identification\n",
    "\n",
    "### 3.1 Determine Optimal Number of Clusters (Elbow Method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method: Calculate inertia for different values of k\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "print(\"Evaluating K-Means for k = 2 to 10...\\n\")\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    sil_score = silhouette_score(X_scaled, kmeans.labels_)\n",
    "    silhouette_scores.append(sil_score)\n",
    "    \n",
    "    print(f\"k={k}: Inertia={kmeans.inertia_:.2f}, Silhouette Score={sil_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Elbow Curve and Silhouette Scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow Curve\n",
    "axes[0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[0].set_ylabel('Inertia (Within-cluster sum of squares)', fontsize=12)\n",
    "axes[0].set_title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(list(k_range))\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mark the \"elbow\" point (k=4 is often a good choice based on diminishing returns)\n",
    "axes[0].axvline(x=4, color='r', linestyle='--', alpha=0.7, label='Suggested k=4')\n",
    "axes[0].legend()\n",
    "\n",
    "# Silhouette Score\n",
    "axes[1].plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score for Different k', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(list(k_range))\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Mark best silhouette score\n",
    "best_k_sil = k_range[np.argmax(silhouette_scores)]\n",
    "axes[1].axvline(x=best_k_sil, color='r', linestyle='--', alpha=0.7, label=f'Best k={best_k_sil}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('elbow_silhouette_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest k based on Silhouette Score: {best_k_sil}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fit K-Means with Optimal k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose optimal k (based on elbow method and silhouette analysis)\n",
    "# We'll use k=4 as it provides a good balance between interpretability and cluster quality\n",
    "\n",
    "OPTIMAL_K = 4\n",
    "\n",
    "print(f\"Fitting K-Means with k={OPTIMAL_K} clusters...\")\n",
    "\n",
    "# Fit final K-Means model\n",
    "kmeans_final = KMeans(n_clusters=OPTIMAL_K, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# Calculate final metrics\n",
    "final_silhouette = silhouette_score(X_scaled, cluster_labels)\n",
    "print(f\"\\nFinal Model Metrics:\")\n",
    "print(f\"  - Inertia: {kmeans_final.inertia_:.2f}\")\n",
    "print(f\"  - Silhouette Score: {final_silhouette:.3f}\")\n",
    "\n",
    "# Cluster distribution\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "print(df['cluster'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Characterize Farming Profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster centroids in original scale\n",
    "cluster_profiles = df.groupby('cluster')[clustering_features].mean()\n",
    "\n",
    "print(\"Cluster Profiles (Mean Values):\")\n",
    "print(\"=\" * 80)\n",
    "cluster_profiles.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create descriptive names for each cluster based on their characteristics\n",
    "def characterize_cluster(row, cluster_id):\n",
    "    \"\"\"Generate a descriptive name for each cluster based on its characteristics.\"\"\"\n",
    "    \n",
    "    # Get overall means for comparison\n",
    "    overall_means = df[clustering_features].mean()\n",
    "    \n",
    "    characteristics = []\n",
    "    \n",
    "    # Check cattle/buffalo (dairy potential)\n",
    "    if row['buffalo_per_holding'] > overall_means['buffalo_per_holding'] * 1.5:\n",
    "        characteristics.append('High Buffalo')\n",
    "    if row['cattle_per_holding'] > overall_means['cattle_per_holding'] * 1.5:\n",
    "        characteristics.append('High Cattle')\n",
    "    \n",
    "    # Check goats (small ruminants)\n",
    "    if row['goat_per_holding'] > overall_means['goat_per_holding'] * 1.5:\n",
    "        characteristics.append('Goat-Focused')\n",
    "    \n",
    "    # Check poultry\n",
    "    if row['poultry_per_holding'] > overall_means['poultry_per_holding'] * 1.5:\n",
    "        characteristics.append('Commercial Poultry')\n",
    "    \n",
    "    # Check irrigation\n",
    "    if row['irrigated_land_pct'] > 60:\n",
    "        characteristics.append('Well-Irrigated')\n",
    "    elif row['irrigated_land_pct'] < 30:\n",
    "        characteristics.append('Rain-fed')\n",
    "    \n",
    "    # Check land holding size\n",
    "    if row['avg_land_holding'] > overall_means['avg_land_holding'] * 1.3:\n",
    "        characteristics.append('Large Holdings')\n",
    "    elif row['avg_land_holding'] < overall_means['avg_land_holding'] * 0.7:\n",
    "        characteristics.append('Small Holdings')\n",
    "    \n",
    "    return ', '.join(characteristics) if characteristics else 'Mixed Farming'\n",
    "\n",
    "# Generate cluster names\n",
    "cluster_names = {}\n",
    "for cluster_id in range(OPTIMAL_K):\n",
    "    row = cluster_profiles.loc[cluster_id]\n",
    "    cluster_names[cluster_id] = characterize_cluster(row, cluster_id)\n",
    "\n",
    "print(\"Cluster Characterization:\")\n",
    "print(\"=\" * 60)\n",
    "for cluster_id, name in cluster_names.items():\n",
    "    count = (df['cluster'] == cluster_id).sum()\n",
    "    print(f\"\\nCluster {cluster_id}: {name}\")\n",
    "    print(f\"  Districts: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more meaningful profile names based on analysis\n",
    "# These names reflect vulnerability and farming system types\n",
    "\n",
    "profile_names = {\n",
    "    0: 'Commercial Agricultural Hubs',\n",
    "    1: 'Subsistence Mixed Farming',\n",
    "    2: 'Highland Pastoral Systems',\n",
    "    3: 'Smallholder Diversified'\n",
    "}\n",
    "\n",
    "# Update based on actual cluster characteristics after viewing the data\n",
    "# We'll refine these after seeing the actual cluster profiles\n",
    "\n",
    "df['profile_name'] = df['cluster'].map(profile_names)\n",
    "\n",
    "print(\"Assigned Profile Names:\")\n",
    "print(df.groupby(['cluster', 'profile_name']).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed cluster analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED CLUSTER ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in range(OPTIMAL_K):\n",
    "    cluster_data = df[df['cluster'] == cluster_id]\n",
    "    print(f\"\\n{'\u2500' * 40}\")\n",
    "    print(f\"CLUSTER {cluster_id}: {profile_names[cluster_id]}\")\n",
    "    print(f\"{'\u2500' * 40}\")\n",
    "    print(f\"Number of Districts: {len(cluster_data)}\")\n",
    "    print(f\"\\nDistricts: {', '.join(cluster_data['district_name'].str.strip().tolist())}\")\n",
    "    print(f\"\\nKey Characteristics:\")\n",
    "    print(f\"  - Avg. Cattle per Holding: {cluster_data['cattle_per_holding'].mean():.2f}\")\n",
    "    print(f\"  - Avg. Buffalo per Holding: {cluster_data['buffalo_per_holding'].mean():.2f}\")\n",
    "    print(f\"  - Avg. Goats per Holding: {cluster_data['goat_per_holding'].mean():.2f}\")\n",
    "    print(f\"  - Avg. Poultry per Holding: {cluster_data['poultry_per_holding'].mean():.2f}\")\n",
    "    print(f\"  - Avg. Land Holding (ha): {cluster_data['avg_land_holding'].mean():.2f}\")\n",
    "    print(f\"  - Irrigated Land (%): {cluster_data['irrigated_land_pct'].mean():.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Part 2: Decision Tree Classification\n",
    "\n",
    "### 4.1 Prepare Data for Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for classification\n",
    "X_clf = df[clustering_features].copy()\n",
    "y_clf = df['cluster'].copy()\n",
    "\n",
    "print(f\"Classification Dataset:\")\n",
    "print(f\"  Features: {X_clf.shape}\")\n",
    "print(f\"  Target distribution:\")\n",
    "print(y_clf.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clf, y_clf, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_clf  # Maintain cluster proportions\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\nTesting set distribution:\")\n",
    "print(y_test.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree Classifier\n",
    "# Using controlled depth for interpretability\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(\n",
    "    max_depth=4,              # Limit depth for interpretability\n",
    "    min_samples_split=5,      # Minimum samples to split a node\n",
    "    min_samples_leaf=2,       # Minimum samples in a leaf\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"\u2713 Decision Tree Classifier trained successfully!\")\n",
    "print(f\"\\nTree depth: {dt_classifier.get_depth()}\")\n",
    "print(f\"Number of leaves: {dt_classifier.get_n_leaves()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Evaluate Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = dt_classifier.predict(X_train)\n",
    "y_test_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2%}\")\n",
    "print(f\"Testing Accuracy:  {test_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Classification Report\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(\"=\" * 60)\n",
    "target_names = [f\"Cluster {i}: {profile_names[i][:20]}\" for i in range(OPTIMAL_K)]\n",
    "print(classification_report(y_test, y_test_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[f'C{i}' for i in range(OPTIMAL_K)],\n",
    "            yticklabels=[f'C{i}' for i in range(OPTIMAL_K)])\n",
    "plt.xlabel('Predicted Cluster', fontsize=12)\n",
    "plt.ylabel('Actual Cluster', fontsize=12)\n",
    "plt.title('Confusion Matrix - Decision Tree Classification', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Visualize Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Decision Tree\n",
    "plt.figure(figsize=(24, 12))\n",
    "plot_tree(\n",
    "    dt_classifier, \n",
    "    feature_names=clustering_features,\n",
    "    class_names=[profile_names[i] for i in range(OPTIMAL_K)],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10,\n",
    "    proportion=True\n",
    ")\n",
    "plt.title('Decision Tree for Livestock Farming Profile Classification', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('decision_tree_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display decision rules in text format\n",
    "print(\"Decision Tree Rules:\")\n",
    "print(\"=\" * 80)\n",
    "tree_rules = export_text(\n",
    "    dt_classifier, \n",
    "    feature_names=clustering_features\n",
    ")\n",
    "print(tree_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Feature Importance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance from Decision Tree\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': clustering_features,\n",
    "    'importance': dt_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'], color='steelblue')\n",
    "plt.xlabel('Feature Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Feature Importance in Decision Tree Classification', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Visualizations\n",
    "\n",
    "### 5.1 Cluster Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster distribution bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of cluster sizes\n",
    "cluster_counts = df['cluster'].value_counts().sort_index()\n",
    "colors = sns.color_palette(\"husl\", OPTIMAL_K)\n",
    "\n",
    "bars = axes[0].bar(cluster_counts.index, cluster_counts.values, color=colors)\n",
    "axes[0].set_xlabel('Cluster', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Districts', fontsize=12)\n",
    "axes[0].set_title('Distribution of Districts Across Clusters', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(range(OPTIMAL_K))\n",
    "axes[0].set_xticklabels([f'C{i}\\n{profile_names[i][:15]}...' for i in range(OPTIMAL_K)])\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, cluster_counts.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                 str(count), ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(cluster_counts.values, labels=[f'C{i}' for i in cluster_counts.index],\n",
    "            autopct='%1.1f%%', colors=colors, explode=[0.02]*OPTIMAL_K)\n",
    "axes[1].set_title('Cluster Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cluster_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Comparison Across Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for key features across clusters\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(clustering_features):\n",
    "    sns.boxplot(x='cluster', y=feature, data=df, ax=axes[idx], palette='husl')\n",
    "    axes[idx].set_title(feature.replace('_', ' ').title(), fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Cluster')\n",
    "    axes[idx].set_ylabel('')\n",
    "\n",
    "plt.suptitle('Feature Distribution Across Clusters', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_boxplots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of cluster centroids (normalized)\n",
    "cluster_means = df.groupby('cluster')[clustering_features].mean()\n",
    "\n",
    "# Normalize for better visualization\n",
    "cluster_means_normalized = (cluster_means - cluster_means.min()) / (cluster_means.max() - cluster_means.min())\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(cluster_means_normalized.T, annot=cluster_means.T.round(2), \n",
    "            cmap='YlOrRd', fmt='.2f', linewidths=0.5,\n",
    "            xticklabels=[f'C{i}: {profile_names[i][:20]}' for i in range(OPTIMAL_K)],\n",
    "            yticklabels=[f.replace('_', ' ').title() for f in clustering_features])\n",
    "plt.title('Cluster Profiles: Feature Comparison Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Cluster (Profile)', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('cluster_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Thematic Map of Nepal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DISTRICT NAME MAPPING FOR THEMATIC MAP\n",
    "# =============================================================================\n",
    "# The GADM shapefile uses different/older district names than our dataset\n",
    "# This cell creates a comprehensive mapping\n",
    "\n",
    "# Our dataset district names (from the CSV files)\n",
    "dataset_districts = [\n",
    "    'taplejung', 'sankhuwasabha', 'solukhumbu', 'okhaldhunga', 'khotang',\n",
    "    'bhojpur', 'dhankuta', 'terhthum', 'panchthar', 'ilam', 'jhapa', 'morang',\n",
    "    'sunsari', 'udayapur', 'saptari', 'sirah', 'dhanusha', 'mahottari', 'sarlahi',\n",
    "    'rautahat', 'bara', 'parsa', 'dolakha', 'sindhupalchok', 'rasuwa', 'dhading',\n",
    "    'nuwakot', 'kathmandu', 'bhaktapur', 'lalitpur', 'kavreplanchok', 'ramechap',\n",
    "    'sindhuli', 'makawanpur', 'chitwan', 'gorkha', 'manag', 'mustang', 'myagdi',\n",
    "    'kaski', 'lamjung', 'tanahu', 'nawalparasi east', 'syangja', 'parbat', 'baglung',\n",
    "    'rukum east', 'rolpa', 'pyuthan', 'gulmi', 'arghakhanchi', 'palpa',\n",
    "    'nawalparasi west', 'rupandehi', 'kapilbastu', 'dang', 'banke', 'bardiya',\n",
    "    'dolpa', 'mugu', 'humla', 'jumla', 'kalikot', 'dailekha', 'jajarkot',\n",
    "    'rukum west', 'salyan', 'surkhet', 'bajura', 'bajhang', 'darchula', 'baitadi',\n",
    "    'dadeldhura', 'doti', 'achham', 'kailali', 'kanchanpur'\n",
    "]\n",
    "\n",
    "# Known GADM Nepal district names (from gadm41_NPL_2.shp)\n",
    "# These are the 77 districts as per GADM 4.1\n",
    "gadm_districts = [\n",
    "    'taplejung', 'sankhuwasabha', 'solukhumbu', 'okhaldhunga', 'khotang',\n",
    "    'bhojpur', 'dhankuta', 'terhathum', 'panchthar', 'ilam', 'jhapa', 'morang',\n",
    "    'sunsari', 'udayapur', 'saptari', 'siraha', 'dhanusha', 'mahottari', 'sarlahi',\n",
    "    'rautahat', 'bara', 'parsa', 'dolakha', 'sindhupalchok', 'rasuwa', 'dhading',\n",
    "    'nuwakot', 'kathmandu', 'bhaktapur', 'lalitpur', 'kavrepalanchok', 'ramechhap',\n",
    "    'sindhuli', 'makwanpur', 'chitawan', 'gorkha', 'manang', 'mustang', 'myagdi',\n",
    "    'kaski', 'lamjung', 'tanahun', 'nawalpur', 'syangja', 'parbat', 'baglung',\n",
    "    'rukum', 'rolpa', 'pyuthan', 'gulmi', 'arghakhanchi', 'palpa',\n",
    "    'parasi', 'rupandehi', 'kapilvastu', 'dang', 'banke', 'bardiya',\n",
    "    'dolpa', 'mugu', 'humla', 'jumla', 'kalikot', 'dailekh', 'jajarkot',\n",
    "    'western rukum', 'salyan', 'surkhet', 'bajura', 'bajhang', 'darchula', 'baitadi',\n",
    "    'dadeldhura', 'doti', 'achham', 'kailali', 'kanchanpur', 'eastern rukum'\n",
    "]\n",
    "\n",
    "# Comprehensive mapping: GADM name -> Our dataset name\n",
    "gadm_to_dataset = {\n",
    "    # Exact matches (most districts)\n",
    "    'taplejung': 'taplejung', 'sankhuwasabha': 'sankhuwasabha', \n",
    "    'solukhumbu': 'solukhumbu', 'okhaldhunga': 'okhaldhunga', 'khotang': 'khotang',\n",
    "    'bhojpur': 'bhojpur', 'dhankuta': 'dhankuta', 'panchthar': 'panchthar', \n",
    "    'ilam': 'ilam', 'jhapa': 'jhapa', 'morang': 'morang', 'sunsari': 'sunsari', \n",
    "    'udayapur': 'udayapur', 'saptari': 'saptari', 'dhanusha': 'dhanusha', \n",
    "    'mahottari': 'mahottari', 'sarlahi': 'sarlahi', 'rautahat': 'rautahat', \n",
    "    'bara': 'bara', 'parsa': 'parsa', 'dolakha': 'dolakha', \n",
    "    'sindhupalchok': 'sindhupalchok', 'rasuwa': 'rasuwa', 'dhading': 'dhading',\n",
    "    'nuwakot': 'nuwakot', 'kathmandu': 'kathmandu', 'bhaktapur': 'bhaktapur', \n",
    "    'lalitpur': 'lalitpur', 'sindhuli': 'sindhuli', 'gorkha': 'gorkha', \n",
    "    'mustang': 'mustang', 'myagdi': 'myagdi', 'kaski': 'kaski', 'lamjung': 'lamjung', \n",
    "    'syangja': 'syangja', 'parbat': 'parbat', 'baglung': 'baglung', 'rolpa': 'rolpa', \n",
    "    'pyuthan': 'pyuthan', 'gulmi': 'gulmi', 'arghakhanchi': 'arghakhanchi', \n",
    "    'palpa': 'palpa', 'rupandehi': 'rupandehi', 'dang': 'dang', 'banke': 'banke', \n",
    "    'bardiya': 'bardiya', 'dolpa': 'dolpa', 'mugu': 'mugu', 'humla': 'humla', \n",
    "    'jumla': 'jumla', 'kalikot': 'kalikot', 'jajarkot': 'jajarkot', \n",
    "    'salyan': 'salyan', 'surkhet': 'surkhet', 'bajura': 'bajura', \n",
    "    'bajhang': 'bajhang', 'darchula': 'darchula', 'baitadi': 'baitadi', \n",
    "    'dadeldhura': 'dadeldhura', 'doti': 'doti', 'achham': 'achham', \n",
    "    'kailali': 'kailali', 'kanchanpur': 'kanchanpur',\n",
    "    \n",
    "    # Name variations that need mapping\n",
    "    'siraha': 'sirah',                    # GADM: Siraha -> Data: Sirah\n",
    "    'terhathum': 'terhthum',              # GADM: Terhathum -> Data: Terhthum\n",
    "    'kavrepalanchok': 'kavreplanchok',    # GADM: Kavrepalanchok -> Data: Kavreplanchok\n",
    "    'ramechhap': 'ramechap',              # GADM: Ramechhap -> Data: Ramechap\n",
    "    'makwanpur': 'makawanpur',            # GADM: Makwanpur -> Data: Makawanpur\n",
    "    'chitawan': 'chitwan',                # GADM: Chitawan -> Data: Chitwan\n",
    "    'manang': 'manag',                    # GADM: Manang -> Data: Manag\n",
    "    'tanahun': 'tanahu',                  # GADM: Tanahun -> Data: Tanahu\n",
    "    'nawalpur': 'nawalparasi east',       # GADM: Nawalpur -> Data: Nawalparasi East\n",
    "    'parasi': 'nawalparasi west',         # GADM: Parasi -> Data: Nawalparasi West\n",
    "    'kapilvastu': 'kapilbastu',           # GADM: Kapilvastu -> Data: Kapilbastu\n",
    "    'dailekh': 'dailekha',                # GADM: Dailekh -> Data: Dailekha\n",
    "    \n",
    "    # Rukum was split into East and West\n",
    "    'rukum': 'rukum west',                # Old Rukum -> Rukum West (larger portion)\n",
    "    'western rukum': 'rukum west',        # GADM Western Rukum\n",
    "    'east rukum': 'rukum east',           # GADM East Rukum  \n",
    "    'eastern rukum': 'rukum east',        # GADM Eastern Rukum\n",
    "    'rukum east': 'rukum east',           # Direct match\n",
    "    'rukum west': 'rukum west',           # Direct match\n",
    "}\n",
    "\n",
    "print(\"District Name Mapping Created:\")\n",
    "print(f\"  Total mappings: {len(gadm_to_dataset)}\")\n",
    "print(f\"  Districts in our dataset: {len(dataset_districts)}\")\n",
    "print(\"\\nKey name variations that will be mapped:\")\n",
    "variations = [(k, v) for k, v in gadm_to_dataset.items() if k != v]\n",
    "for gadm, data in variations:\n",
    "    print(f\"  GADM: '{gadm}' -> Dataset: '{data}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Nepal shapefile if geopandas is available\n",
    "if GEOPANDAS_AVAILABLE:\n",
    "    try:\n",
    "        import urllib.request\n",
    "        import zipfile\n",
    "        import os\n",
    "        \n",
    "        # GADM 4.1 Nepal shapefile structure:\n",
    "        # - gadm41_NPL_0.shp = Country level\n",
    "        # - gadm41_NPL_1.shp = Province level (7 provinces)\n",
    "        # - gadm41_NPL_2.shp = Zone level (14 zones - OLD admin division)\n",
    "        # - gadm41_NPL_3.shp = District level (77 districts) <- We need this!\n",
    "        \n",
    "        shapefile_url = \"https://geodata.ucdavis.edu/gadm/gadm4.1/shp/gadm41_NPL_shp.zip\"\n",
    "        \n",
    "        if not os.path.exists('nepal_districts'):\n",
    "            os.makedirs('nepal_districts', exist_ok=True)\n",
    "            \n",
    "            print(\"Downloading Nepal shapefile from GADM...\")\n",
    "            print(\"This includes all administrative levels (country, province, zone, district)\")\n",
    "            urllib.request.urlretrieve(shapefile_url, 'nepal_districts/nepal.zip')\n",
    "            \n",
    "            with zipfile.ZipFile('nepal_districts/nepal.zip', 'r') as zip_ref:\n",
    "                zip_ref.extractall('nepal_districts')\n",
    "            print(\"\u2713 Shapefile downloaded and extracted!\")\n",
    "            \n",
    "            # List available shapefiles\n",
    "            shp_files = [f for f in os.listdir('nepal_districts') if f.endswith('.shp')]\n",
    "            print(f\"\\nAvailable shapefiles: {shp_files}\")\n",
    "        else:\n",
    "            print(\"Shapefile already downloaded.\")\n",
    "            shp_files = [f for f in os.listdir('nepal_districts') if f.endswith('.shp')]\n",
    "            print(f\"Available shapefiles: {shp_files}\")\n",
    "        \n",
    "        SHAPEFILE_AVAILABLE = True\n",
    "        print(\"\\n\u2713 We will use gadm41_NPL_3.shp for district-level mapping\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not download shapefile: {e}\")\n",
    "        print(\"Map visualization will use alternative method.\")\n",
    "        SHAPEFILE_AVAILABLE = False\n",
    "else:\n",
    "    print(\"geopandas not available - skipping shapefile download\")\n",
    "    SHAPEFILE_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create thematic map if shapefile is available\n",
    "if GEOPANDAS_AVAILABLE and SHAPEFILE_AVAILABLE:\n",
    "    try:\n",
    "        # GADM Nepal structure: NAME_1=Province, NAME_2=Zone, NAME_3=District\n",
    "        # We need NAME_3 for districts!\n",
    "        nepal_gdf = gpd.read_file('nepal_districts/gadm41_NPL_3.shp')  # District level (NAME_3)\n",
    "\n",
    "        print(f\"Shapefile loaded with {len(nepal_gdf)} features\")\n",
    "        print(f\"Available columns: {nepal_gdf.columns.tolist()}\")\n",
    "\n",
    "        # Check which NAME column has district-level data\n",
    "        if 'NAME_3' in nepal_gdf.columns:\n",
    "            district_col = 'NAME_3'\n",
    "        elif 'NAME_2' in nepal_gdf.columns:\n",
    "            district_col = 'NAME_2'\n",
    "        else:\n",
    "            district_col = 'NAME_1'\n",
    "\n",
    "        # Show sample district names from shapefile\n",
    "        shapefile_raw_names = sorted([str(n).strip().lower() for n in nepal_gdf[district_col].dropna().unique()])\n",
    "        print(f\"\\nSample districts in shapefile ({len(shapefile_raw_names)} total):\")\n",
    "        for i, name in enumerate(shapefile_raw_names[:15], 1):\n",
    "            print(f\"  {i:2}. {name}\")\n",
    "        if len(shapefile_raw_names) > 15:\n",
    "            print(f\"  ... and {len(shapefile_raw_names) - 15} more\")\n",
    "\n",
    "        # Use the comprehensive mapping from cell 59 (gadm_to_dataset)\n",
    "        def map_shapefile_to_dataset(name):\n",
    "            # Map GADM shapefile district name to our dataset district name.\n",
    "            if pd.isna(name):\n",
    "                return name\n",
    "            cleaned = str(name).strip().lower()\n",
    "            return gadm_to_dataset.get(cleaned, cleaned)\n",
    "\n",
    "        # Apply mapping to shapefile\n",
    "        nepal_gdf['district_clean'] = nepal_gdf[district_col].apply(map_shapefile_to_dataset)\n",
    "\n",
    "        # Get data districts (already cleaned)\n",
    "        data_district_set = set(df['district_clean'].dropna().unique())\n",
    "        shapefile_mapped_set = set(nepal_gdf['district_clean'].dropna().unique())\n",
    "\n",
    "        # Calculate matching stats\n",
    "        matched = shapefile_mapped_set & data_district_set\n",
    "        unmatched_in_shapefile = shapefile_mapped_set - data_district_set\n",
    "        unmatched_in_data = data_district_set - shapefile_mapped_set\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"DISTRICT MATCHING RESULTS:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\u2713 Successfully matched: {len(matched)} districts\")\n",
    "\n",
    "        if unmatched_in_shapefile:\n",
    "            print(f\"\\n\u26a0 Shapefile districts not in data ({len(unmatched_in_shapefile)}):\")\n",
    "            for d in sorted(list(unmatched_in_shapefile)[:10]):\n",
    "                print(f\"    - {d}\")\n",
    "            if len(unmatched_in_shapefile) > 10:\n",
    "                print(f\"    ... and {len(unmatched_in_shapefile) - 10} more\")\n",
    "\n",
    "        if unmatched_in_data:\n",
    "            print(f\"\\n\u26a0 Data districts not in shapefile ({len(unmatched_in_data)}):\")\n",
    "            for d in sorted(list(unmatched_in_data)[:10]):\n",
    "                print(f\"    - {d}\")\n",
    "            if len(unmatched_in_data) > 10:\n",
    "                print(f\"    ... and {len(unmatched_in_data) - 10} more\")\n",
    "\n",
    "        print(f\"\\nMatching rate: {len(matched)/len(data_district_set)*100:.1f}%\")\n",
    "\n",
    "        # Dissolve shapefile by district to get district-level polygons\n",
    "        nepal_districts = nepal_gdf.dissolve(by='district_clean', as_index=False)\n",
    "        print(f\"\\nDissolved to {len(nepal_districts)} district polygons\")\n",
    "\n",
    "        # Merge with our clustered data\n",
    "        nepal_districts = nepal_districts.merge(\n",
    "            df[['district_clean', 'cluster', 'profile_name']],\n",
    "            on='district_clean',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Count how many districts have cluster data\n",
    "        districts_with_data = nepal_districts['cluster'].notna().sum()\n",
    "        print(f\"Districts with cluster data in map: {districts_with_data}/{len(nepal_districts)}\")\n",
    "\n",
    "        # Custom color palette (colorblind-friendly)\n",
    "        from matplotlib.colors import ListedColormap\n",
    "        cluster_colors = ['#1b9e77', '#d95f02', '#7570b3', '#e7298a']  # C0..C3\n",
    "        cmap = ListedColormap(cluster_colors)\n",
    "        missing_color = '#d9d9d9'\n",
    "\n",
    "        # Create the map only if we have data\n",
    "        if districts_with_data > 0:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(18, 12))\n",
    "\n",
    "            # Plot with cluster colors\n",
    "            nepal_districts.plot(\n",
    "                column='cluster',\n",
    "                categorical=True,\n",
    "                legend=False,\n",
    "                cmap=cmap,\n",
    "                edgecolor='black',\n",
    "                linewidth=0.5,\n",
    "                ax=ax,\n",
    "                missing_kwds={'color': missing_color, 'label': 'No Data'}\n",
    "            )\n",
    "\n",
    "            ax.set_title('Thematic Map: Livestock Farming Profiles of Nepal',\n",
    "                        fontsize=18, fontweight='bold', pad=20)\n",
    "            ax.set_axis_off()\n",
    "\n",
    "            # Add custom legend with profile names\n",
    "            from matplotlib.patches import Patch\n",
    "            legend_elements = [\n",
    "                Patch(facecolor=cluster_colors[i], edgecolor='black', label=f'C{i}: {profile_names[i]}')\n",
    "                for i in range(OPTIMAL_K)\n",
    "            ]\n",
    "            legend_elements.append(Patch(facecolor=missing_color, edgecolor='black', label='No Data'))\n",
    "            ax.legend(handles=legend_elements, loc='lower left', fontsize=10, title='Farming Profiles')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('nepal_thematic_map.png', dpi=200, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "            print(\"\\n\u2713 Thematic map created successfully with improved colors!\")\n",
    "        else:\n",
    "            print(\"\\n\u26a0 No matching districts found. Skipping map creation.\")\n",
    "            SHAPEFILE_AVAILABLE = False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating map: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nFalling back to alternative visualization.\")\n",
    "        SHAPEFILE_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative visualization if map is not available\n",
    "if not GEOPANDAS_AVAILABLE or not SHAPEFILE_AVAILABLE:\n",
    "    print(\"Creating alternative district-cluster visualization...\\n\")\n",
    "    \n",
    "    # Create a text-based geographic representation\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    # Sort districts by cluster\n",
    "    df_sorted = df.sort_values('cluster')\n",
    "    \n",
    "    # Create a scatter plot with districts labeled\n",
    "    colors = sns.color_palette('husl', OPTIMAL_K)\n",
    "    \n",
    "    for cluster_id in range(OPTIMAL_K):\n",
    "        cluster_data = df_sorted[df_sorted['cluster'] == cluster_id]\n",
    "        ax.scatter([cluster_id] * len(cluster_data), \n",
    "                   range(len(cluster_data)),\n",
    "                   c=[colors[cluster_id]], \n",
    "                   s=200, \n",
    "                   label=f'C{cluster_id}: {profile_names[cluster_id]}')\n",
    "        \n",
    "        # Add district names\n",
    "        for idx, (_, row) in enumerate(cluster_data.iterrows()):\n",
    "            ax.annotate(row['district_name'].strip()[:12], \n",
    "                       (cluster_id, idx),\n",
    "                       fontsize=7, ha='center', va='center')\n",
    "    \n",
    "    ax.set_xlabel('Cluster', fontsize=12)\n",
    "    ax.set_ylabel('Districts', fontsize=12)\n",
    "    ax.set_title('District Distribution Across Farming Profiles', fontsize=14, fontweight='bold')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.set_xticks(range(OPTIMAL_K))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('district_cluster_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table showing districts by cluster\n",
    "print(\"\\nDistricts by Farming Profile:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in range(OPTIMAL_K):\n",
    "    cluster_districts = df[df['cluster'] == cluster_id]['district_name'].str.strip().tolist()\n",
    "    print(f\"\\n{profile_names[cluster_id]} (Cluster {cluster_id}):\")\n",
    "    print(f\"  Count: {len(cluster_districts)} districts\")\n",
    "    print(f\"  Districts: {', '.join(cluster_districts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Results Summary and Policy Recommendations\n",
    "\n",
    "### 6.1 Summary of Findings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "summary_data = []\n",
    "\n",
    "for cluster_id in range(OPTIMAL_K):\n",
    "    cluster_df = df[df['cluster'] == cluster_id]\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Profile': profile_names[cluster_id],\n",
    "        'Cluster': cluster_id,\n",
    "        'Num Districts': len(cluster_df),\n",
    "        'Avg Cattle/Holding': cluster_df['cattle_per_holding'].mean(),\n",
    "        'Avg Buffalo/Holding': cluster_df['buffalo_per_holding'].mean(),\n",
    "        'Avg Goats/Holding': cluster_df['goat_per_holding'].mean(),\n",
    "        'Avg Poultry/Holding': cluster_df['poultry_per_holding'].mean(),\n",
    "        'Avg Land (ha)': cluster_df['avg_land_holding'].mean(),\n",
    "        'Irrigated %': cluster_df['irrigated_land_pct'].mean(),\n",
    "        'Livestock/ha': cluster_df['livestock_density_per_ha'].mean()\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"SUMMARY TABLE: Livestock Farming Profiles in Nepal\")\n",
    "print(\"=\" * 100)\n",
    "print(summary_df.round(2).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary to CSV\n",
    "summary_df.round(2).to_csv('cluster_summary.csv', index=False)\n",
    "df.to_csv('districts_with_clusters.csv', index=False)\n",
    "\n",
    "print(\"\\n\u2713 Results exported to:\")\n",
    "print(\"  - cluster_summary.csv\")\n",
    "print(\"  - districts_with_clusters.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Vulnerability Assessment and Policy Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate policy recommendations based on cluster characteristics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"POLICY RECOMMENDATIONS BY FARMING PROFILE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "recommendations = {\n",
    "    0: {\n",
    "        'vulnerability': 'LOW-MEDIUM',\n",
    "        'strengths': [\n",
    "            'Good irrigation infrastructure',\n",
    "            'Diversified livestock portfolio',\n",
    "            'Access to markets'\n",
    "        ],\n",
    "        'challenges': [\n",
    "            'Market price volatility',\n",
    "            'Disease outbreak risks in dense populations'\n",
    "        ],\n",
    "        'recommendations': [\n",
    "            'Establish livestock insurance schemes',\n",
    "            'Develop cold storage and processing facilities',\n",
    "            'Implement disease surveillance systems',\n",
    "            'Promote cooperative marketing'\n",
    "        ]\n",
    "    },\n",
    "    1: {\n",
    "        'vulnerability': 'HIGH',\n",
    "        'strengths': [\n",
    "            'Traditional farming knowledge',\n",
    "            'Low input dependency'\n",
    "        ],\n",
    "        'challenges': [\n",
    "            'Limited irrigation',\n",
    "            'Small land holdings',\n",
    "            'Climate vulnerability',\n",
    "            'Limited market access'\n",
    "        ],\n",
    "        'recommendations': [\n",
    "            'Implement small-scale irrigation projects',\n",
    "            'Provide goat/sheep farming subsidies',\n",
    "            'Establish community pasture management',\n",
    "            'Create mobile veterinary services',\n",
    "            'Develop micro-credit programs'\n",
    "        ]\n",
    "    },\n",
    "    2: {\n",
    "        'vulnerability': 'MEDIUM-HIGH',\n",
    "        'strengths': [\n",
    "            'Large cattle holdings',\n",
    "            'Highland adapted breeds',\n",
    "            'Pastoral traditions'\n",
    "        ],\n",
    "        'challenges': [\n",
    "            'Harsh climate conditions',\n",
    "            'Limited infrastructure',\n",
    "            'Seasonal migration patterns'\n",
    "        ],\n",
    "        'recommendations': [\n",
    "            'Support traditional transhumance practices',\n",
    "            'Improve mountain road connectivity',\n",
    "            'Establish highland breed conservation programs',\n",
    "            'Create seasonal veterinary camps',\n",
    "            'Develop high-altitude fodder cultivation'\n",
    "        ]\n",
    "    },\n",
    "    3: {\n",
    "        'vulnerability': 'MEDIUM',\n",
    "        'strengths': [\n",
    "            'Diversified farming systems',\n",
    "            'Moderate irrigation access',\n",
    "            'Mixed livestock-crop integration'\n",
    "        ],\n",
    "        'challenges': [\n",
    "            'Land fragmentation',\n",
    "            'Labor migration',\n",
    "            'Limited mechanization'\n",
    "        ],\n",
    "        'recommendations': [\n",
    "            'Promote integrated farming systems',\n",
    "            'Support small-scale dairy cooperatives',\n",
    "            'Provide training in improved animal husbandry',\n",
    "            'Develop local feed production',\n",
    "            'Create farmer producer organizations'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for cluster_id in range(OPTIMAL_K):\n",
    "    rec = recommendations[cluster_id]\n",
    "    print(f\"\\n{'\u2500' * 60}\")\n",
    "    print(f\"CLUSTER {cluster_id}: {profile_names[cluster_id]}\")\n",
    "    print(f\"{'\u2500' * 60}\")\n",
    "    print(f\"\\nVulnerability Level: {rec['vulnerability']}\")\n",
    "    \n",
    "    print(f\"\\nStrengths:\")\n",
    "    for s in rec['strengths']:\n",
    "        print(f\"  \u2713 {s}\")\n",
    "    \n",
    "    print(f\"\\nChallenges:\")\n",
    "    for c in rec['challenges']:\n",
    "        print(f\"  \u2717 {c}\")\n",
    "    \n",
    "    print(f\"\\nPolicy Recommendations:\")\n",
    "    for i, r in enumerate(rec['recommendations'], 1):\n",
    "        print(f\"  {i}. {r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Key Findings from Decision Tree Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree analysis reveals interpretable rules for classifying districts into vulnerability profiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize key decision rules\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY DECISION RULES FOR PROFILE CLASSIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "The Decision Tree model reveals the following key factors for classifying\n",
    "agricultural vulnerability profiles:\n",
    "\n",
    "1. PRIMARY SPLITTING FACTORS (Most Important):\n",
    "\"\"\")\n",
    "\n",
    "# Display top features\n",
    "top_features = feature_importance.sort_values('importance', ascending=False).head(3)\n",
    "for _, row in top_features.iterrows():\n",
    "    print(f\"   \u2022 {row['feature'].replace('_', ' ').title()}: {row['importance']:.2%} importance\")\n",
    "\n",
    "print(\"\"\"\n",
    "2. INTERPRETATION:\n",
    "   - Districts can be classified into vulnerability profiles using simple rules\n",
    "   - The most discriminating features relate to livestock density and irrigation\n",
    "   - This provides an actionable framework for policy targeting\n",
    "\n",
    "3. MODEL PERFORMANCE:\n",
    "\"\"\")\n",
    "print(f\"   - Training Accuracy: {train_accuracy:.2%}\")\n",
    "print(f\"   - Testing Accuracy: {test_accuracy:.2%}\")\n",
    "print(f\"   - Tree Depth: {dt_classifier.get_depth()} (interpretable)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "================================================================================\n",
    "                              CONCLUSION\n",
    "================================================================================\n",
    "\n",
    "This study successfully applied machine learning techniques to identify and \n",
    "characterize distinct livestock farming profiles across Nepal's 77 districts.\n",
    "\n",
    "KEY ACHIEVEMENTS:\n",
    "\n",
    "1. CLUSTERING ANALYSIS:\n",
    "   - Identified {0} distinct farming profiles using K-Means clustering\n",
    "   - Achieved silhouette score of {1:.3f}, indicating good cluster separation\n",
    "   - Profiles range from commercial agricultural hubs to subsistence farming\n",
    "\n",
    "2. CLASSIFICATION MODEL:\n",
    "   - Decision Tree classifier achieved {2:.1%} test accuracy\n",
    "   - Model provides interpretable rules for profile classification\n",
    "   - Key factors: livestock density, irrigation, and land characteristics\n",
    "\n",
    "3. POLICY IMPLICATIONS:\n",
    "   - Framework enables targeted agricultural interventions\n",
    "   - Vulnerability assessment guides resource allocation\n",
    "   - Evidence-based approach for extension services\n",
    "\n",
    "RECOMMENDATIONS FOR FUTURE WORK:\n",
    "   - Incorporate temporal data for trend analysis\n",
    "   - Add climate vulnerability indicators\n",
    "   - Include market access and infrastructure data\n",
    "   - Validate profiles with ground-truth surveys\n",
    "\n",
    "================================================================================\n",
    "\"\"\".format(OPTIMAL_K, final_silhouette, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FILES GENERATED:\")\n",
    "print(\"=\" * 60)\n",
    "output_files = [\n",
    "    ('cluster_summary.csv', 'Summary statistics for each farming profile'),\n",
    "    ('districts_with_clusters.csv', 'Full dataset with cluster assignments'),\n",
    "    ('elbow_silhouette_analysis.png', 'Cluster selection analysis'),\n",
    "    ('confusion_matrix.png', 'Classification model evaluation'),\n",
    "    ('decision_tree_visualization.png', 'Decision tree diagram'),\n",
    "    ('feature_importance.png', 'Feature importance ranking'),\n",
    "    ('cluster_distribution.png', 'Cluster size distribution'),\n",
    "    ('feature_boxplots.png', 'Feature comparison across clusters'),\n",
    "    ('cluster_heatmap.png', 'Cluster profile heatmap')\n",
    "]\n",
    "\n",
    "for filename, description in output_files:\n",
    "    print(f\"  \u2022 {filename}: {description}\")\n",
    "\n",
    "print(\"\\n\u2713 Analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# END OF ANALYSIS\n",
    "# =============================================================================\n",
    "# This notebook has successfully completed:\n",
    "# 1. Data loading and preprocessing\n",
    "# 2. Feature engineering with 14 derived features\n",
    "# 3. K-Means clustering to identify 4 farming profiles\n",
    "# 4. Decision Tree classification for interpretable vulnerability modeling\n",
    "# 5. Comprehensive visualizations including thematic mapping\n",
    "# 6. Policy recommendations for each farming profile\n",
    "#\n",
    "# All outputs have been saved to CSV and PNG files.\n",
    "print(\"=\" * 60)\n",
    "print(\"NOTEBOOK EXECUTION COMPLETE\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}